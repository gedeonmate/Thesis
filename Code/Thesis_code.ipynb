{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e380238",
   "metadata": {},
   "source": [
    "# Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf50e85",
   "metadata": {},
   "source": [
    "Creating one big frequency table of Hungarian words. \n",
    "Sources: HNC (http://corpus.nytud.hu/mnsz/index_eng.html) and HWC (http://mokk.bme.hu/resources/webcorpus/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6704617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "colnames=['word','freq','1','2','3']\n",
    "df1 = pd.read_csv(\"web2.2-freq-sorted.txt\", sep='\\t',header=None, encoding='ISO-8859-2',names=colnames)\n",
    "df1 = df1[['word','freq']]\n",
    "df1 = df1.groupby(['word']).sum()\n",
    "df1 = df1.sort_values('freq', ascending=False).reset_index()\n",
    "\n",
    "colnames = ['word','1','2','3','4','5','6','freq']\n",
    "df2 = pd.read_csv(\"hnc-1.3-wordfreq.txt\", sep='\\t',header=None,encoding=\"latin-1\",names=colnames)\n",
    "df2 = df2[['word','freq']]\n",
    "df2 = df2.groupby(['word']).sum()\n",
    "df2 = df2.sort_values('freq', ascending=False).reset_index()\n",
    "\n",
    "# merge the two dataframes based on 'word'\n",
    "merged_df = pd.merge(df1, df2, on='word', how='outer')\n",
    "\n",
    "# group by 'word' and sum 'freq'\n",
    "grouped_df = merged_df.groupby('word')['freq_x', 'freq_y'].sum().reset_index()\n",
    "\n",
    "# set 'freq' to the sum of 'freq_x' and 'freq_y'\n",
    "grouped_df['freq'] = grouped_df['freq_x'] + grouped_df['freq_y']\n",
    "\n",
    "# drop the 'freq_x' and 'freq_y' columns\n",
    "grouped_df = grouped_df.drop(['freq_x', 'freq_y'], axis=1)\n",
    "\n",
    "# sort the dataframe by 'freq' in descending order\n",
    "grouped_df = grouped_df.sort_values(by='freq', ascending=False)\n",
    "\n",
    "# reset the index of the dataframe\n",
    "grouped_df = grouped_df.reset_index(drop=True)\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7642c523",
   "metadata": {},
   "source": [
    "Loading in the analogy questions available on: http://corpus.nytud.hu/efnilex-vect/data/questions-words-hu.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a40b8f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary set of the analogy questions\n",
    "with open('analogy.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    # read in the lines and split them into words\n",
    "    words = set()\n",
    "    for line in f:\n",
    "        try:\n",
    "            w1, w2, w3, w4 = line.strip().split()\n",
    "            words.update([w1, w2, w3, w4])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Vocabulary set of the static embedding from efnilex (http://corpus.nytud.hu/efnilex-vect/)\n",
    "# Creation of 'keys.txt'\n",
    "with open('keys.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    # read in the lines and split them into words\n",
    "    efnilex = set()\n",
    "    for line in f:\n",
    "        word = line.strip()\n",
    "        efnilex.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b006532d",
   "metadata": {},
   "source": [
    "Restricting the vocabulary to words that appear at least 100 times according to the frequency table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8665bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = grouped_df[grouped_df.freq > 100]\n",
    "efni = filtered[filtered['word'].isin(efnilex)]\n",
    "efni = efni.sort_values('freq', ascending=False).reset_index()\n",
    "efni = efni.drop(columns=['index'])\n",
    "\n",
    "restricted = set(efni['word'])\n",
    "with open('restricted_vocab.txt', 'w',encoding=\"utf-8\") as f:\n",
    "    for item in restricted:\n",
    "        f.write('%s\\n' % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc68a5be",
   "metadata": {},
   "source": [
    "# Extracting word vectors from huBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14190d2",
   "metadata": {},
   "source": [
    "Extracting word vectors from the first layer, using mean pooling (with attention mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882ab014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SZTAKI-HLT/hubert-base-cc\")\n",
    "model = AutoModel.from_pretrained(\"SZTAKI-HLT/hubert-base-cc\")\n",
    "\n",
    "# Importing the vocabulary to a list\n",
    "my_file = open('restricted_vocab.txt', 'r',encoding=\"utf-8\")\n",
    "data = my_file.read()\n",
    "keys = data.replace('\\n', ' ').split(\" \")\n",
    "my_file.close()\n",
    "\n",
    "d = dict()\n",
    "# The batch size 128 proved to be the most efficient\n",
    "for i in tqdm(range(0,3800)):\n",
    "    words=keys[(i)*128:(i+1)*128]\n",
    "    encoded = tokenizer(words, padding=True, return_tensors=\"pt\")\n",
    "    bert_output = model(**encoded, output_hidden_states=True)\n",
    "    embedding_output = bert_output['hidden_states'][0]\n",
    "    # Mask out the padding tokens\n",
    "    attention_mask = encoded['attention_mask']\n",
    "    mask = attention_mask.unsqueeze(-1).expand(embedding_output.size()).float()\n",
    "    masked_output = embedding_output * mask\n",
    "\n",
    "    # Compute the mean of the non-padding tokens along the sequence axis\n",
    "    pooled_output = torch.sum(masked_output, 1) / torch.clamp(torch.sum(mask, 1), min=1e-9)\n",
    "    for i, word in enumerate(words):\n",
    "        # Get the embedding for the ith word\n",
    "        embedding = pooled_output[i]\n",
    "        # Convert the embedding to a numpy array\n",
    "        embedding = np.asarray(embedding.detach())\n",
    "        # Add the embedding to the dictionary\n",
    "        d[word] = embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68786277",
   "metadata": {},
   "source": [
    "Extracting word vectors from all layers, mean pooling them separately, then mean pooling the resulted tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e740ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a dictionary to store the embeddings\n",
    "d = dict()\n",
    "\n",
    "# Loop over the words in batches\n",
    "for i in tqdm(range(3000,4000)):\n",
    "    words = keys[(i)*128:(i+1)*128]\n",
    "\n",
    "    # Encode the words using the tokenizer\n",
    "    encoded = tokenizer(words, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Get the outputs of all the layers of the model\n",
    "    hubert_output = model(**encoded, output_hidden_states=True)\n",
    "    hidden_states = hubert_output['hidden_states']\n",
    "\n",
    "    # Loop over the layers and apply attention-based pooling\n",
    "    layer_pooled_outputs = []\n",
    "    for layer_output in hidden_states:\n",
    "        # Compute the attention scores\n",
    "        attention_scores = torch.matmul(layer_output, layer_output.transpose(-1, -2))\n",
    "\n",
    "        # Mask out the padding tokens\n",
    "        attention_mask = encoded['attention_mask']\n",
    "        mask = attention_mask.unsqueeze(1) * attention_mask.unsqueeze(2)\n",
    "        attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Normalize the attention scores\n",
    "        attention_probs = torch.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # Apply attention-based pooling\n",
    "        layer_pooled_output = torch.matmul(attention_probs, layer_output)\n",
    "\n",
    "        # Mask out the padding tokens\n",
    "        attention_mask = encoded['attention_mask']\n",
    "        mask = attention_mask.unsqueeze(-1).expand(layer_pooled_output.size()).float()\n",
    "        layer_pooled_output = layer_pooled_output * mask\n",
    "\n",
    "        # Compute the mean of the non-padding tokens along the sequence axis\n",
    "        layer_pooled_output = torch.sum(layer_pooled_output, 1) / torch.clamp(torch.sum(mask, 1), min=1e-9)\n",
    "\n",
    "        layer_pooled_outputs.append(layer_pooled_output)\n",
    "\n",
    "    # Stack the layer pooled outputs into a tensor\n",
    "    stacked_layer_pooled_outputs = torch.stack(layer_pooled_outputs, dim=1)\n",
    "\n",
    "    # Compute mean pooling across the tensor's second dimension (the layer dimension)\n",
    "    pooled_output = torch.mean(stacked_layer_pooled_outputs, dim=1)\n",
    "\n",
    "    # Create a dictionary to store the embeddings\n",
    "    for i, word in enumerate(words):\n",
    "        # Get the embedding for the ith word\n",
    "        embedding = pooled_output[i]\n",
    "        # Convert the embedding to a numpy array\n",
    "        embedding = np.asarray(embedding.detach())\n",
    "        # Add the embedding to the dictionary\n",
    "        d[word] = embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b00482",
   "metadata": {},
   "source": [
    "Storing a dictionary as a word2vec compatible binary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3152b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_word2vec_format(fname, vocab, vector_size, binary=True):\n",
    "    total_vec = len(vocab)\n",
    "    with gensim.utils.open(fname, 'wb') as fout:\n",
    "        print(total_vec, vector_size)\n",
    "        fout.write(gensim.utils.to_utf8(\"%s %s\\n\" % (total_vec, vector_size)))\n",
    "        # store in sorted order: most frequent words at the topÂ \n",
    "        for word, row in tqdm(vocab.items()):\n",
    "            if binary:\n",
    "                row = row.astype(np.float32)\n",
    "                fout.write(gensim.utils.to_utf8(word) + b\" \" + row.tostring())\n",
    "            else:\n",
    "                fout.write(gensim.utils.to_utf8(\"%s %s\\n\" % (word, ' '.join(repr(val) for val in row))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3a8ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_word2vec_format(binary=True, fname='test_99.bin', vocab=d, vector_size=768)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7455c5ef",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed16bba6",
   "metadata": {},
   "source": [
    "With 'd' dictionary containing n (in our case 768) dimensional word embeddings, the next code creates a d1 dictionary that contains embeddings for the same words but with 300 (can be easily changed) dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65b8f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "import pickle\n",
    "    \n",
    "l=list(d.values())\n",
    "ma=np.array(l)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def flip_signs(A, B):\n",
    "    \"\"\"\n",
    "    utility function for resolving the sign ambiguity in SVD\n",
    "    http://stats.stackexchange.com/q/34396/115202\n",
    "    \"\"\"\n",
    "    signs = np.sign(A) * np.sign(B)\n",
    "    return A, B * signs\n",
    "\n",
    "\n",
    "# Let the data matrix X be of n x p size,\n",
    "# where n is the number of samples and p is the number of variables\n",
    "X = ma\n",
    "# Let us assume that it is centered\n",
    "X -= np.mean(X, axis=0)\n",
    "\n",
    "# the p x p covariance matrix\n",
    "C = np.cov(X, rowvar=False)\n",
    "print(\"C = \\n\", C)\n",
    "# C is a symmetric matrix and so it can be diagonalized:\n",
    "l, principal_axes = la.eig(C)\n",
    "# sort results wrt. eigenvalues\n",
    "idx = l.argsort()[::-1]\n",
    "l, principal_axes = l[idx], principal_axes[:, idx]\n",
    "# the eigenvalues in decreasing order\n",
    "print(\"l = \\n\", l)\n",
    "# a matrix of eigenvectors (each column is an eigenvector)\n",
    "print(\"V = \\n\", principal_axes)\n",
    "# projections of X on the principal axes are called principal components\n",
    "principal_components = X.dot(principal_axes)\n",
    "print(\"Y = \\n\", principal_components)\n",
    "\n",
    "# we now perform singular value decomposition of X\n",
    "# \"economy size\" (or \"thin\") SVD\n",
    "U, s, Vt = la.svd(X, full_matrices=False)\n",
    "V = Vt.T\n",
    "S = np.diag(s)\n",
    " \n",
    "# PCA to reduce dimensionality to k\n",
    "d1 = dict()\n",
    "k=300\n",
    "PC_k = principal_components[:, 0:k]\n",
    "US_k = U[:, 0:k].dot(S[0:k, 0:k])\n",
    "j=0\n",
    "for i in d.keys():\n",
    "    d1[i]=US_k[j]\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d110f46",
   "metadata": {},
   "source": [
    "Extracting sentences from Hungarian Wikipedia that contains the selected ambigous words\n",
    "The Hungarian Wikipedia is available here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f967c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-made set of Hungarian ambigous words\n",
    "with open('../hun_ambigous.txt',encoding='UTF-8') as file:\n",
    "    words = [line.rstrip() for line in file]\n",
    "\n",
    "# The Hungarian Wikipedia separated in lines\n",
    "with open('../hun_wiki/hun_wiki1.txt',encoding='UTF-8') as file:\n",
    "    lines1 = [line.rstrip() for line in file]\n",
    "with open('../hun_wiki/hun_wiki2.txt',encoding='UTF-8') as file:\n",
    "    lines2 = [line.rstrip() for line in file]\n",
    "with open('../hun_wiki/hun_wiki3.txt',encoding='UTF-8') as file:\n",
    "    lines3 = [line.rstrip() for line in file]\n",
    "with open('../hun_wiki/hun_wiki4.txt',encoding='UTF-8') as file:\n",
    "    lines4 = [line.rstrip() for line in file]\n",
    "with open('../hun_wiki/hun_wiki5.txt',encoding='UTF-8') as file:\n",
    "    lines5 = [line.rstrip() for line in file]\n",
    "lines=lines1+lines2+lines3+lines4+lines5\n",
    "\n",
    "# Listing all the appearances of the ambigous words in Hungarian Wikipedia\n",
    "separated=[ [] for _ in range(51) ]\n",
    "for i,word in enumerate(words):\n",
    "    for j in lines:\n",
    "        if ' '+word+' ' in j:\n",
    "            separated[i].append(j)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d79323",
   "metadata": {},
   "source": [
    "Selecting example sentences for both sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8343a166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next code were used to print out 100 random appearance of an ambigous word \n",
    "import random\n",
    "\n",
    "random.sample(separated[0],100)\n",
    "\n",
    "# From here, I selected 15-15 sentences for both senses in the following form\n",
    "toll1 = ['A konyhÃ¡ban ezalatt a lÃ¡nyok megprÃ³bÃ¡lkoznak az Ã¡tokhoz szÃ¼ksÃ©ges bÃ¡jital Ã¶sszeÃ¡llÃ­tÃ¡sÃ¡val, azonban nÃ©hÃ¡ny gyÃ³gynÃ¶vÃ©ny Ã©s egy foltos bagolytÃ³l szÃ¡rmazÃ³ toll hiÃ¡nyzik az otthoni kÃ©szletbÅl.',\n",
    "'TehÃ¡t ha a mÃ©rleg serpenyÅje, a szÃ­v Ã©s a toll egyensÃºlyban Ã¡ll, akkor a lÃ©lek bebocsÃ¡ttatÃ¡st nyert Ozirisz halhatatlan birodalmÃ¡ba.',\n",
    " 'De ha vÃ¡kuumban vÃ©gezzÃ¼k a kÃ­sÃ©rletet, akkor a toll Ã©s a kalapÃ¡cs ugyanolyan gyorsan esik a talaj felÃ©.',\n",
    " 'HiÃ¡nyzik a koponya nagyobb rÃ©sze, lÃ¡bai hosszÃºak Ã©s erÅsek, tollazata nem lÃ¡thatÃ³, csak tÃ¼zetes vizsgÃ¡latok utÃ¡n lehetett kijelenteni, hogy az alkarcsonthoz nÃ©hÃ¡ny toll is csatlakozik.',\n",
    " 'A homlokÃ¡n barnÃ¡s-zÃ¶ld csÃ­k talÃ¡lhatÃ³ a viaszhÃ¡rtyÃ¡jÃ¡n pedig pÃ¡r darab piros toll is lehet, de ez csak az idÅsebb egyedekre jellemzÅ.',\n",
    "  'SzÃ¡rnya 39-45 centimÃ©ter, farka 27-30 centimÃ©ter, a kÃ©t kÃ¶zÃ©psÅ toll kivÃ©telÃ©vel mindegyik faroktollon feltÅ±nÅen szÃ©les vÃ¶rÃ¶s sÃ¡v hÃºzÃ³dik.',\n",
    "  'A tollszÃ¡r csomÃ³kat a toll tÃ¼szÅkhÃ¶z kapcsolÃ³dÃ³ Ã­nszalagok hoztÃ¡k lÃ©tre, Ã©s mivel tÃ¼szÅkbÅl nem alakultak ki pikkelyek, a szerzÅk kizÃ¡rtÃ¡k annak lehetÅsÃ©gÃ©t, hogy a karon hosszÃº, jelzÃ©sre szolgÃ¡lÃ³ pikkelyek helyezkedtek el.',\n",
    " 'Egy jÃ³ Ã¡llapotban megÅrzÅdÃ¶tt rÃ©szleges csontvÃ¡z alapjÃ¡n ismert, melynek farkÃ¡hoz nÃ©gy hosszÃº, szÃ¡rbÃ³l Ã©s zÃ¡szlÃ³kbÃ³l Ã¡llÃ³ toll lenyomata kapcsolÃ³dik.',\n",
    " 'A nekem tollazata hasonlÃ³, de a hÃ­m csÅrre valamivel hosszabb mint a tojÃ³, Ã©s fejÃ©n sÃ¶tÃ©tebb toll talÃ¡lhatÃ³.',\n",
    " 'A fÅzet mÃ¡r majdnem kÃ©szen van a konyhÃ¡ban, s mÃ¡r csak a toll hiÃ¡nyzik az Ã¶sszetevÅk kÃ¶zÃ¼l.',\n",
    " 'A jelentÅs nemi kÃ©talakÃºsÃ¡g jellemzi a Sphecotheres-fajokat: a hÃ­mek begyi Ã©s hasi rÃ©szei olajzÃ¶ldek, a fejÃ¼k fekete, pofÃ¡jukon pedig a toll nÃ©lkÃ¼li bÅr Ã©lÃ©nk vÃ¶rÃ¶s; a tojÃ³k kevÃ©sbÃ© feltÅ±nÅk, felÃ¼l barnÃ¡sak, alul pedig fehÃ©rek sÃ¶tÃ©t csÃ­kozÃ¡ssal, a toll nÃ©lkÃ¼li bÅrÃ¼k Ã©s csÅrÃ¼k szÃ¼rkÃ©sfekete.',\n",
    " 'A fiatal hÃ­mek fejecskÃ©jÃ©n az elsÅ nÃ©hÃ¡ny vÃ¶rÃ¶s toll mÃ¡r a 3-4 hÃ³napos korban megjelenik.',\n",
    " 'A nÃ¡szidÅszakban szÃ­nes tollak nÅnek a hÃ­mek fejÃ©n, toll nÃ©lkÃ¼li pofÃ¡jÃ¡n a bÅre narancssÃ¡rgÃ¡vÃ¡ vÃ¡lik, begye megfeketedik, Ã©s nyaka kÃ¶rÃ¼l a nyakfodorhoz hasonlÃ³ gallÃ©r alakul ki.',\n",
    " 'RÃ¶vid csÅre csonkÃ¡nak hat, Ã©s tÃ¶vÃ©t toll borÃ­tja.',\n",
    " 'A szÃ¡rnyak Ã©s a farok sÃ¶tÃ©t barna, a legszÃ©lsÅ farok toll fehÃ©r.'\n",
    "]\n",
    "toll2 = ['A toll olyan Ã­rÃ³eszkÃ¶z, amely a tintÃ¡nak az Ã­rÃ³felÃ¼letre (leggyakrabban papÃ­rra) valÃ³ felhordÃ¡sÃ¡ra alkalmas Ã­rÃ¡s vagy rajzolÃ¡s ÃºtjÃ¡n.',\n",
    " 'A jegyzettÃ¶mb papÃ­rbÃ³l, vagy lemoshatÃ³ mÅ±anyagbÃ³l kÃ©szÃ¼l, mÃ­g a toll egy speciÃ¡lis gÃ¶mb formÃ¡jÃº kupakkal van ellÃ¡tva, hogy megakadÃ¡lyozza a bÃ­rÃ³t a sÃ©rÃ¼lÃ©stÅl.',\n",
    " 'CsupÃ¡n a vonal hordozÃ³ja: a tus, ceruza vagy toll nyoma Ã©s a papÃ­r a materiÃ¡lis elem.\"',\n",
    "  'Egy toll alakÃº eszkÃ¶z segÃ­ti a felhasznÃ¡lÃ³t a kÃ©szÃ¼lÃ©k hasznÃ¡latÃ¡ban.',\n",
    " 'Mivel tÃ¶bbnyire jobbrÃ³l balra Ã­rjÃ¡k, praktikusabb bal kÃ©zzel Ã­rni, ellenkezÅ esetben ugyanis a toll hegye Ã¡tlyukasztja a papÃ­rt, az Ã­rÃ³ kÃ©z pedig elmaszatolja a tintÃ¡t.',\n",
    " 'Amikor a toll hegye Ã©rintkezett a tÃ¡blÃ¡val, akkor a mÃ¡trixban az adott ponton elhelyezkedÅ mikrofon Ã©rzÃ©kelte a hangot, Ã©s tovÃ¡bbÃ­totta az adatot a szÃ¡mÃ­tÃ³gÃ©p felÃ©.',\n",
    " 'HabÃ¡r a szemÃ©lyi szÃ¡mÃ­tÃ³gÃ©p Ã©s a billentyÅ±zet hasznÃ¡lata egyre inkÃ¡bb teret nyert mindennapi Ã©letÃ¼nkben, a toll mÃ©g mindig a hÃ©tkÃ¶znapok legfontosabb Ã­rÃ³eszkÃ¶ze maradt.',\n",
    " 'Hamarosan felismerte, hogy a golyÃ³ formÃ¡jÃ¡nak precizitÃ¡sa nagyban befolyÃ¡solja a toll minÅsÃ©gÃ©t, ezÃ©rt szigorÃº ellenÅrzÃ©st vezetett be.',\n",
    " 'A kereskedelmi forgalomba kerÃ¼lÅ kÃ©szÃ¼lÃ©kek kÃ¶zÃ¼l az 1989-ben piacra dobott Grid Systems GRiDPAD-ja volt az, ami legjobban egyesÃ­tette mindazt formÃ¡ban, amit alapvetÅen a tÃ¡blagÃ©pekben ma is meghatÃ¡rozÃ³nak tartunk, de a 10 hÃ¼velykes kijelzÅ itt mÃ©g monokrÃ³m, Ã©s beÃ©pÃ­tett toll segÃ­tsÃ©gÃ©vel volt lehetÅsÃ©g az adatbevitelre.',\n",
    " 'Az Å±rtollat 1967-tÅl hasznÃ¡lhattÃ¡k az amerikai Å±rhajÃ³sok, 1969-ben mÃ¡r a szovjet Å±rÃ¼gynÃ¶ksÃ©g is Ã©rdeklÅdÃ¶tt a toll irÃ¡nt.',\n",
    "  'A toll mÃ¡rkanevÃ©ben utalÃ¡st tett csalÃ¡dnevÃ©re is, illetve angolul a gÃ¶rdÃ¼lÅ megoldÃ¡sÃº tollra is, Ã­gy lett az kontinentÃ¡lis EurÃ³pa elsÅ sajÃ¡t golyÃ³stollÃ¡nak neve âGOâPEN.',\n",
    "  '1938-ban BÃ­rÃ³ LÃ¡szlÃ³, magyar ÃºjsÃ¡gÃ­rÃ³ feltalÃ¡lt egy olyan Ã­rÃ³eszkÃ¶zt, amelyben a tollhegy vÃ©gÃ©n egy aprÃ³ golyÃ³ forgott szabadon Ã©s vitte fel a toll belsejÃ©ben tÃ¡rolt tintÃ¡t a papÃ­rra.',\n",
    " 'TalÃ¡lmÃ¡nyÃ¡t folyamatosan tÃ¶kÃ©letesÃ­tve 1906-ban London Ã©szaki rÃ©szÃ©n Tottenhamban lÃ©trehozta a stencilgÃ©pek, festÃ©kek, hengerek Ã©s a kerekes toll gyÃ¡rtÃ¡sÃ¡ra szakosodott Ã¼zemÃ©t.',\n",
    "  'MÃ©gis, a nagy technikai fejlÅdÃ©s ellenÃ©re â ami a toll Ã©s papÃ­r hasznÃ¡latÃ¡tÃ³l a szÃ¡mÃ­tÃ³gÃ©p-hÃ¡lÃ³zatokhoz vezetett â az egyes cÃ©gek kÃ¶nyvelÃ©sÃ©nek lÃ©nyeges meghatÃ¡rozÃ³ja, a kÃ¶nyvelÃ©s kÃ¶ltsÃ©ge mÃ©gsem csÃ¶kkent jelentÅsen.',\n",
    " 'A gÃ©p elsÅ konstrukciÃ³ja fÅleg azÃ©rt volt nagyon kÃ¼lÃ¶nleges, mert az mÃ©g akkor kÃ©szÃ¼lt el, amikor mÃ©g nem volt olyan lÃ©ptetÅmotor, amit a toll mozgatÃ¡sÃ¡hoz alkalmazni lehetett volna.'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a65566",
   "metadata": {},
   "source": [
    "Extracting vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6990eee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "db=0\n",
    "sent = ['A jelentÅs nemi kÃ©talakÃºsÃ¡g jellemzi a Sphecotheres-fajokat: a hÃ­mek begyi Ã©s hasi rÃ©szei olajzÃ¶ldek, a fejÃ¼k fekete, pofÃ¡jukon pedig a toll nÃ©lkÃ¼li bÅr Ã©lÃ©nk vÃ¶rÃ¶s; a tojÃ³k kevÃ©sbÃ© feltÅ±nÅk, felÃ¼l barnÃ¡sak, alul pedig fehÃ©rek sÃ¶tÃ©t csÃ­kozÃ¡ssal, a toll nÃ©lkÃ¼li bÅrÃ¼k Ã©s csÅrÃ¼k szÃ¼rkÃ©sfekete.',\n",
    " 'A fiatal hÃ­mek fejecskÃ©jÃ©n az elsÅ nÃ©hÃ¡ny vÃ¶rÃ¶s toll mÃ¡r a 3-4 hÃ³napos korban megjelenik.',\n",
    " 'A nÃ¡szidÅszakban szÃ­nes tollak nÅnek a hÃ­mek fejÃ©n, toll nÃ©lkÃ¼li pofÃ¡jÃ¡n a bÅre narancssÃ¡rgÃ¡vÃ¡ vÃ¡lik, begye megfeketedik, Ã©s nyaka kÃ¶rÃ¼l a nyakfodorhoz hasonlÃ³ gallÃ©r alakul ki.',\n",
    " 'RÃ¶vid csÅre csonkÃ¡nak hat, Ã©s tÃ¶vÃ©t toll borÃ­tja.',\n",
    " 'A szÃ¡rnyak Ã©s a farok sÃ¶tÃ©t barna, a legszÃ©lsÅ farok toll fehÃ©r.'\n",
    "    ]\n",
    "for sentence in sent:\n",
    "    # List of example sentences containing the ambiguous word\n",
    "    ambiguous_word = \"toll\"  # The ambiguous word to extract vector for\n",
    "    # Initialize list to store encoded sentences\n",
    "    encoded_sentences = []\n",
    "\n",
    "    indices = []\n",
    "\n",
    "    # Tokenize the sentence and find the position of the ambiguous word\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    word_indices = [i for i, token in enumerate(tokens) if token == ambiguous_word]\n",
    "\n",
    "    # Encode the sentence and mark the word position with special tokens [CLS] and [SEP]\n",
    "    tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    word_indices = [idx + 1 for idx in word_indices]  # Shift word indices by 1 to account for [CLS] token\n",
    "    indices.append(word_indices[:1])\n",
    "    encoded_sentences.append(torch.tensor(token_ids))\n",
    "\n",
    "    # Perform mean pooling on all layers\n",
    "    all_layers = []\n",
    "    for encoded_sentence, word_indice in zip(encoded_sentences, indices):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(encoded_sentence.unsqueeze(0), output_hidden_states=True)\n",
    "            hidden_states = outputs['hidden_states']\n",
    "            pooled_output = torch.mean(torch.stack(hidden_states), dim=0)\n",
    "            all_layers.append(pooled_output.squeeze()[word_indice])\n",
    "\n",
    "    # Perform mean pooling on the resulting vectors\n",
    "    word_vector = torch.mean(torch.stack(all_layers), dim=0)\n",
    "\n",
    "    # Convert the word vector to a numpy array\n",
    "    word_vector = word_vector.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd683b5",
   "metadata": {},
   "source": [
    "Evaluating (First with Euclidian distance, then with cosine similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2258da58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "w='toll'\n",
    "\n",
    "# Given arrays\n",
    "array1 = d_sep[f'{w}1']\n",
    "array2 = d_sep[f'{w}2']\n",
    "array3 = d[f'{w}']\n",
    "\n",
    "# Construct the coefficient matrix\n",
    "A = np.vstack([array1, array2]).T\n",
    "\n",
    "# Solve the least squares problem\n",
    "coefficients, residuals, _, _ = np.linalg.lstsq(A, array3, rcond=None)\n",
    "\n",
    "# Calculate the linear combination\n",
    "linear_combination = np.dot(A, coefficients)\n",
    "\n",
    "# Calculate the distance between linear_combination and array3\n",
    "distance = np.linalg.norm(linear_combination - array3)\n",
    "\n",
    "print(\"Coefficients:\", coefficients)\n",
    "print(\"Distance:\", distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f45d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = np.linspace(0.0,10,num=1000)\n",
    "c2 = np.linspace(0.0,10,num=1000)\n",
    "\n",
    "w = 'rÃ¡k'\n",
    "# Assuming you have three numpy arrays: arr1, arr2, arr3\n",
    "arr1 = d_sep[f'{w}1']\n",
    "arr2 = d_sep[f'{w}2']\n",
    "arr3 = d[f'{w}']\n",
    "maximum = 0\n",
    "a=0\n",
    "b=0\n",
    "for i in c1:\n",
    "    for j in c2:\n",
    "        weighted_sum = i * arr1 + j * arr2\n",
    "        similarity = cosine_similarity(weighted_sum.reshape(1, -1), arr3.reshape(1, -1))\n",
    "        if similarity > maximum:\n",
    "            maximum = similarity\n",
    "            a = i\n",
    "            b = j\n",
    "print(similarity, a, b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
