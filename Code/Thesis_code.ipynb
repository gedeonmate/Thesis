{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e380238",
   "metadata": {},
   "source": [
    "# Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf50e85",
   "metadata": {},
   "source": [
    "Creating one big frequency table of Hungarian words. \n",
    "Sources: HNC (http://corpus.nytud.hu/mnsz/index_eng.html) and HWC (http://mokk.bme.hu/resources/webcorpus/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6704617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "colnames=['word','freq','1','2','3']\n",
    "df1 = pd.read_csv(\"web2.2-freq-sorted.txt\", sep='\\t',header=None, encoding='ISO-8859-2',names=colnames)\n",
    "df1 = df1[['word','freq']]\n",
    "df1 = df1.groupby(['word']).sum()\n",
    "df1 = df1.sort_values('freq', ascending=False).reset_index()\n",
    "\n",
    "colnames = ['word','1','2','3','4','5','6','freq']\n",
    "df2 = pd.read_csv(\"hnc-1.3-wordfreq.txt\", sep='\\t',header=None,encoding=\"latin-1\",names=colnames)\n",
    "df2 = df2[['word','freq']]\n",
    "df2 = df2.groupby(['word']).sum()\n",
    "df2 = df2.sort_values('freq', ascending=False).reset_index()\n",
    "\n",
    "# merge the two dataframes based on 'word'\n",
    "merged_df = pd.merge(df1, df2, on='word', how='outer')\n",
    "\n",
    "# group by 'word' and sum 'freq'\n",
    "grouped_df = merged_df.groupby('word')['freq_x', 'freq_y'].sum().reset_index()\n",
    "\n",
    "# set 'freq' to the sum of 'freq_x' and 'freq_y'\n",
    "grouped_df['freq'] = grouped_df['freq_x'] + grouped_df['freq_y']\n",
    "\n",
    "# drop the 'freq_x' and 'freq_y' columns\n",
    "grouped_df = grouped_df.drop(['freq_x', 'freq_y'], axis=1)\n",
    "\n",
    "# sort the dataframe by 'freq' in descending order\n",
    "grouped_df = grouped_df.sort_values(by='freq', ascending=False)\n",
    "\n",
    "# reset the index of the dataframe\n",
    "grouped_df = grouped_df.reset_index(drop=True)\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7642c523",
   "metadata": {},
   "source": [
    "Loading in the analogy questions available on: http://corpus.nytud.hu/efnilex-vect/data/questions-words-hu.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a40b8f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary set of the analogy questions\n",
    "with open('analogy.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    # read in the lines and split them into words\n",
    "    words = set()\n",
    "    for line in f:\n",
    "        try:\n",
    "            w1, w2, w3, w4 = line.strip().split()\n",
    "            words.update([w1, w2, w3, w4])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Vocabulary set of the static embedding from efnilex (http://corpus.nytud.hu/efnilex-vect/)\n",
    "# Creation of 'keys.txt'\n",
    "with open('keys.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    # read in the lines and split them into words\n",
    "    efnilex = set()\n",
    "    for line in f:\n",
    "        word = line.strip()\n",
    "        efnilex.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b006532d",
   "metadata": {},
   "source": [
    "Restricting the vocabulary to words that appear at least 100 times according to the frequency table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8665bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = grouped_df[grouped_df.freq > 100]\n",
    "efni = filtered[filtered['word'].isin(efnilex)]\n",
    "efni = efni.sort_values('freq', ascending=False).reset_index()\n",
    "efni = efni.drop(columns=['index'])\n",
    "\n",
    "restricted = set(efni['word'])\n",
    "with open('restricted_vocab.txt', 'w',encoding=\"utf-8\") as f:\n",
    "    for item in restricted:\n",
    "        f.write('%s\\n' % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc68a5be",
   "metadata": {},
   "source": [
    "# Extracting word vectors from huBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14190d2",
   "metadata": {},
   "source": [
    "Extracting word vectors from the first layer, using mean pooling (with attention mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882ab014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SZTAKI-HLT/hubert-base-cc\")\n",
    "model = AutoModel.from_pretrained(\"SZTAKI-HLT/hubert-base-cc\")\n",
    "\n",
    "# Importing the vocabulary to a list\n",
    "my_file = open('restricted_vocab.txt', 'r',encoding=\"utf-8\")\n",
    "data = my_file.read()\n",
    "keys = data.replace('\\n', ' ').split(\" \")\n",
    "my_file.close()\n",
    "\n",
    "d = dict()\n",
    "# The batch size 128 proved to be the most efficient\n",
    "for i in tqdm(range(0,3800)):\n",
    "    words=keys[(i)*128:(i+1)*128]\n",
    "    encoded = tokenizer(words, padding=True, return_tensors=\"pt\")\n",
    "    bert_output = model(**encoded, output_hidden_states=True)\n",
    "    embedding_output = bert_output['hidden_states'][0]\n",
    "    # Mask out the padding tokens\n",
    "    attention_mask = encoded['attention_mask']\n",
    "    mask = attention_mask.unsqueeze(-1).expand(embedding_output.size()).float()\n",
    "    masked_output = embedding_output * mask\n",
    "\n",
    "    # Compute the mean of the non-padding tokens along the sequence axis\n",
    "    pooled_output = torch.sum(masked_output, 1) / torch.clamp(torch.sum(mask, 1), min=1e-9)\n",
    "    for i, word in enumerate(words):\n",
    "        # Get the embedding for the ith word\n",
    "        embedding = pooled_output[i]\n",
    "        # Convert the embedding to a numpy array\n",
    "        embedding = np.asarray(embedding.detach())\n",
    "        # Add the embedding to the dictionary\n",
    "        d[word] = embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68786277",
   "metadata": {},
   "source": [
    "Extracting word vectors from all layers, mean pooling them separately, then mean pooling the resulted tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e740ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a dictionary to store the embeddings\n",
    "d = dict()\n",
    "\n",
    "# Loop over the words in batches\n",
    "for i in tqdm(range(3000,4000)):\n",
    "    words = keys[(i)*128:(i+1)*128]\n",
    "\n",
    "    # Encode the words using the tokenizer\n",
    "    encoded = tokenizer(words, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Get the outputs of all the layers of the model\n",
    "    hubert_output = model(**encoded, output_hidden_states=True)\n",
    "    hidden_states = hubert_output['hidden_states']\n",
    "\n",
    "    # Loop over the layers and apply attention-based pooling\n",
    "    layer_pooled_outputs = []\n",
    "    for layer_output in hidden_states:\n",
    "        # Compute the attention scores\n",
    "        attention_scores = torch.matmul(layer_output, layer_output.transpose(-1, -2))\n",
    "\n",
    "        # Mask out the padding tokens\n",
    "        attention_mask = encoded['attention_mask']\n",
    "        mask = attention_mask.unsqueeze(1) * attention_mask.unsqueeze(2)\n",
    "        attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Normalize the attention scores\n",
    "        attention_probs = torch.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # Apply attention-based pooling\n",
    "        layer_pooled_output = torch.matmul(attention_probs, layer_output)\n",
    "\n",
    "        # Mask out the padding tokens\n",
    "        attention_mask = encoded['attention_mask']\n",
    "        mask = attention_mask.unsqueeze(-1).expand(layer_pooled_output.size()).float()\n",
    "        layer_pooled_output = layer_pooled_output * mask\n",
    "\n",
    "        # Compute the mean of the non-padding tokens along the sequence axis\n",
    "        layer_pooled_output = torch.sum(layer_pooled_output, 1) / torch.clamp(torch.sum(mask, 1), min=1e-9)\n",
    "\n",
    "        layer_pooled_outputs.append(layer_pooled_output)\n",
    "\n",
    "    # Stack the layer pooled outputs into a tensor\n",
    "    stacked_layer_pooled_outputs = torch.stack(layer_pooled_outputs, dim=1)\n",
    "\n",
    "    # Compute mean pooling across the tensor's second dimension (the layer dimension)\n",
    "    pooled_output = torch.mean(stacked_layer_pooled_outputs, dim=1)\n",
    "\n",
    "    # Create a dictionary to store the embeddings\n",
    "    for i, word in enumerate(words):\n",
    "        # Get the embedding for the ith word\n",
    "        embedding = pooled_output[i]\n",
    "        # Convert the embedding to a numpy array\n",
    "        embedding = np.asarray(embedding.detach())\n",
    "        # Add the embedding to the dictionary\n",
    "        d[word] = embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b00482",
   "metadata": {},
   "source": [
    "Storing a dictionary as a word2vec compatible binary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3152b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_word2vec_format(fname, vocab, vector_size, binary=True):\n",
    "    total_vec = len(vocab)\n",
    "    with gensim.utils.open(fname, 'wb') as fout:\n",
    "        print(total_vec, vector_size)\n",
    "        fout.write(gensim.utils.to_utf8(\"%s %s\\n\" % (total_vec, vector_size)))\n",
    "        # store in sorted order: most frequent words at the top \n",
    "        for word, row in tqdm(vocab.items()):\n",
    "            if binary:\n",
    "                row = row.astype(np.float32)\n",
    "                fout.write(gensim.utils.to_utf8(word) + b\" \" + row.tostring())\n",
    "            else:\n",
    "                fout.write(gensim.utils.to_utf8(\"%s %s\\n\" % (word, ' '.join(repr(val) for val in row))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3a8ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_word2vec_format(binary=True, fname='test_99.bin', vocab=d, vector_size=768)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7455c5ef",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed16bba6",
   "metadata": {},
   "source": [
    "With 'd' dictionary containing n (in our case 768) dimensional word embeddings, the next code creates a d1 dictionary that contains embeddings for the same words but with 300 (can be easily changed) dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65b8f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "import pickle\n",
    "    \n",
    "l=list(d.values())\n",
    "ma=np.array(l)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def flip_signs(A, B):\n",
    "    \"\"\"\n",
    "    utility function for resolving the sign ambiguity in SVD\n",
    "    http://stats.stackexchange.com/q/34396/115202\n",
    "    \"\"\"\n",
    "    signs = np.sign(A) * np.sign(B)\n",
    "    return A, B * signs\n",
    "\n",
    "\n",
    "# Let the data matrix X be of n x p size,\n",
    "# where n is the number of samples and p is the number of variables\n",
    "X = ma\n",
    "# Let us assume that it is centered\n",
    "X -= np.mean(X, axis=0)\n",
    "\n",
    "# the p x p covariance matrix\n",
    "C = np.cov(X, rowvar=False)\n",
    "print(\"C = \\n\", C)\n",
    "# C is a symmetric matrix and so it can be diagonalized:\n",
    "l, principal_axes = la.eig(C)\n",
    "# sort results wrt. eigenvalues\n",
    "idx = l.argsort()[::-1]\n",
    "l, principal_axes = l[idx], principal_axes[:, idx]\n",
    "# the eigenvalues in decreasing order\n",
    "print(\"l = \\n\", l)\n",
    "# a matrix of eigenvectors (each column is an eigenvector)\n",
    "print(\"V = \\n\", principal_axes)\n",
    "# projections of X on the principal axes are called principal components\n",
    "principal_components = X.dot(principal_axes)\n",
    "print(\"Y = \\n\", principal_components)\n",
    "\n",
    "# we now perform singular value decomposition of X\n",
    "# \"economy size\" (or \"thin\") SVD\n",
    "U, s, Vt = la.svd(X, full_matrices=False)\n",
    "V = Vt.T\n",
    "S = np.diag(s)\n",
    " \n",
    "# PCA to reduce dimensionality to k\n",
    "d1 = dict()\n",
    "k=300\n",
    "PC_k = principal_components[:, 0:k]\n",
    "US_k = U[:, 0:k].dot(S[0:k, 0:k])\n",
    "j=0\n",
    "for i in d.keys():\n",
    "    d1[i]=US_k[j]\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d110f46",
   "metadata": {},
   "source": [
    "Extracting sentences from Hungarian Wikipedia that contains the selected ambigous words\n",
    "The Hungarian Wikipedia is available here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f967c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-made set of Hungarian ambigous words\n",
    "with open('../hun_ambigous.txt',encoding='UTF-8') as file:\n",
    "    words = [line.rstrip() for line in file]\n",
    "\n",
    "# The Hungarian Wikipedia separated in lines\n",
    "with open('../hun_wiki/hun_wiki1.txt',encoding='UTF-8') as file:\n",
    "    lines1 = [line.rstrip() for line in file]\n",
    "with open('../hun_wiki/hun_wiki2.txt',encoding='UTF-8') as file:\n",
    "    lines2 = [line.rstrip() for line in file]\n",
    "with open('../hun_wiki/hun_wiki3.txt',encoding='UTF-8') as file:\n",
    "    lines3 = [line.rstrip() for line in file]\n",
    "with open('../hun_wiki/hun_wiki4.txt',encoding='UTF-8') as file:\n",
    "    lines4 = [line.rstrip() for line in file]\n",
    "with open('../hun_wiki/hun_wiki5.txt',encoding='UTF-8') as file:\n",
    "    lines5 = [line.rstrip() for line in file]\n",
    "lines=lines1+lines2+lines3+lines4+lines5\n",
    "\n",
    "# Listing all the appearances of the ambigous words in Hungarian Wikipedia\n",
    "separated=[ [] for _ in range(51) ]\n",
    "for i,word in enumerate(words):\n",
    "    for j in lines:\n",
    "        if ' '+word+' ' in j:\n",
    "            separated[i].append(j)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d79323",
   "metadata": {},
   "source": [
    "Selecting example sentences for both sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8343a166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next code were used to print out 100 random appearance of an ambigous word \n",
    "import random\n",
    "\n",
    "random.sample(separated[0],100)\n",
    "\n",
    "# From here, I selected 15-15 sentences for both senses in the following form\n",
    "toll1 = ['A konyhában ezalatt a lányok megpróbálkoznak az átokhoz szükséges bájital összeállításával, azonban néhány gyógynövény és egy foltos bagolytól származó toll hiányzik az otthoni készletből.',\n",
    "'Tehát ha a mérleg serpenyője, a szív és a toll egyensúlyban áll, akkor a lélek bebocsáttatást nyert Ozirisz halhatatlan birodalmába.',\n",
    " 'De ha vákuumban végezzük a kísérletet, akkor a toll és a kalapács ugyanolyan gyorsan esik a talaj felé.',\n",
    " 'Hiányzik a koponya nagyobb része, lábai hosszúak és erősek, tollazata nem látható, csak tüzetes vizsgálatok után lehetett kijelenteni, hogy az alkarcsonthoz néhány toll is csatlakozik.',\n",
    " 'A homlokán barnás-zöld csík található a viaszhártyáján pedig pár darab piros toll is lehet, de ez csak az idősebb egyedekre jellemző.',\n",
    "  'Szárnya 39-45 centiméter, farka 27-30 centiméter, a két középső toll kivételével mindegyik faroktollon feltűnően széles vörös sáv húzódik.',\n",
    "  'A tollszár csomókat a toll tüszőkhöz kapcsolódó ínszalagok hozták létre, és mivel tüszőkből nem alakultak ki pikkelyek, a szerzők kizárták annak lehetőségét, hogy a karon hosszú, jelzésre szolgáló pikkelyek helyezkedtek el.',\n",
    " 'Egy jó állapotban megőrződött részleges csontváz alapján ismert, melynek farkához négy hosszú, szárból és zászlókból álló toll lenyomata kapcsolódik.',\n",
    " 'A nekem tollazata hasonló, de a hím csőrre valamivel hosszabb mint a tojó, és fején sötétebb toll található.',\n",
    " 'A főzet már majdnem készen van a konyhában, s már csak a toll hiányzik az összetevők közül.',\n",
    " 'A jelentős nemi kétalakúság jellemzi a Sphecotheres-fajokat: a hímek begyi és hasi részei olajzöldek, a fejük fekete, pofájukon pedig a toll nélküli bőr élénk vörös; a tojók kevésbé feltűnők, felül barnásak, alul pedig fehérek sötét csíkozással, a toll nélküli bőrük és csőrük szürkésfekete.',\n",
    " 'A fiatal hímek fejecskéjén az első néhány vörös toll már a 3-4 hónapos korban megjelenik.',\n",
    " 'A nászidőszakban színes tollak nőnek a hímek fején, toll nélküli pofáján a bőre narancssárgává válik, begye megfeketedik, és nyaka körül a nyakfodorhoz hasonló gallér alakul ki.',\n",
    " 'Rövid csőre csonkának hat, és tövét toll borítja.',\n",
    " 'A szárnyak és a farok sötét barna, a legszélső farok toll fehér.'\n",
    "]\n",
    "toll2 = ['A toll olyan íróeszköz, amely a tintának az írófelületre (leggyakrabban papírra) való felhordására alkalmas írás vagy rajzolás útján.',\n",
    " 'A jegyzettömb papírból, vagy lemosható műanyagból készül, míg a toll egy speciális gömb formájú kupakkal van ellátva, hogy megakadályozza a bírót a sérüléstől.',\n",
    " 'Csupán a vonal hordozója: a tus, ceruza vagy toll nyoma és a papír a materiális elem.\"',\n",
    "  'Egy toll alakú eszköz segíti a felhasználót a készülék használatában.',\n",
    " 'Mivel többnyire jobbról balra írják, praktikusabb bal kézzel írni, ellenkező esetben ugyanis a toll hegye átlyukasztja a papírt, az író kéz pedig elmaszatolja a tintát.',\n",
    " 'Amikor a toll hegye érintkezett a táblával, akkor a mátrixban az adott ponton elhelyezkedő mikrofon érzékelte a hangot, és továbbította az adatot a számítógép felé.',\n",
    " 'Habár a személyi számítógép és a billentyűzet használata egyre inkább teret nyert mindennapi életünkben, a toll még mindig a hétköznapok legfontosabb íróeszköze maradt.',\n",
    " 'Hamarosan felismerte, hogy a golyó formájának precizitása nagyban befolyásolja a toll minőségét, ezért szigorú ellenőrzést vezetett be.',\n",
    " 'A kereskedelmi forgalomba kerülő készülékek közül az 1989-ben piacra dobott Grid Systems GRiDPAD-ja volt az, ami legjobban egyesítette mindazt formában, amit alapvetően a táblagépekben ma is meghatározónak tartunk, de a 10 hüvelykes kijelző itt még monokróm, és beépített toll segítségével volt lehetőség az adatbevitelre.',\n",
    " 'Az űrtollat 1967-től használhatták az amerikai űrhajósok, 1969-ben már a szovjet űrügynökség is érdeklődött a toll iránt.',\n",
    "  'A toll márkanevében utalást tett családnevére is, illetve angolul a gördülő megoldású tollra is, így lett az kontinentális Európa első saját golyóstollának neve „GO”PEN.',\n",
    "  '1938-ban Bíró László, magyar újságíró feltalált egy olyan íróeszközt, amelyben a tollhegy végén egy apró golyó forgott szabadon és vitte fel a toll belsejében tárolt tintát a papírra.',\n",
    " 'Találmányát folyamatosan tökéletesítve 1906-ban London északi részén Tottenhamban létrehozta a stencilgépek, festékek, hengerek és a kerekes toll gyártására szakosodott üzemét.',\n",
    "  'Mégis, a nagy technikai fejlődés ellenére – ami a toll és papír használatától a számítógép-hálózatokhoz vezetett – az egyes cégek könyvelésének lényeges meghatározója, a könyvelés költsége mégsem csökkent jelentősen.',\n",
    " 'A gép első konstrukciója főleg azért volt nagyon különleges, mert az még akkor készült el, amikor még nem volt olyan léptetőmotor, amit a toll mozgatásához alkalmazni lehetett volna.'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a65566",
   "metadata": {},
   "source": [
    "Extracting vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6990eee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "db=0\n",
    "sent = ['A jelentős nemi kétalakúság jellemzi a Sphecotheres-fajokat: a hímek begyi és hasi részei olajzöldek, a fejük fekete, pofájukon pedig a toll nélküli bőr élénk vörös; a tojók kevésbé feltűnők, felül barnásak, alul pedig fehérek sötét csíkozással, a toll nélküli bőrük és csőrük szürkésfekete.',\n",
    " 'A fiatal hímek fejecskéjén az első néhány vörös toll már a 3-4 hónapos korban megjelenik.',\n",
    " 'A nászidőszakban színes tollak nőnek a hímek fején, toll nélküli pofáján a bőre narancssárgává válik, begye megfeketedik, és nyaka körül a nyakfodorhoz hasonló gallér alakul ki.',\n",
    " 'Rövid csőre csonkának hat, és tövét toll borítja.',\n",
    " 'A szárnyak és a farok sötét barna, a legszélső farok toll fehér.'\n",
    "    ]\n",
    "for sentence in sent:\n",
    "    # List of example sentences containing the ambiguous word\n",
    "    ambiguous_word = \"toll\"  # The ambiguous word to extract vector for\n",
    "    # Initialize list to store encoded sentences\n",
    "    encoded_sentences = []\n",
    "\n",
    "    indices = []\n",
    "\n",
    "    # Tokenize the sentence and find the position of the ambiguous word\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    word_indices = [i for i, token in enumerate(tokens) if token == ambiguous_word]\n",
    "\n",
    "    # Encode the sentence and mark the word position with special tokens [CLS] and [SEP]\n",
    "    tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    word_indices = [idx + 1 for idx in word_indices]  # Shift word indices by 1 to account for [CLS] token\n",
    "    indices.append(word_indices[:1])\n",
    "    encoded_sentences.append(torch.tensor(token_ids))\n",
    "\n",
    "    # Perform mean pooling on all layers\n",
    "    all_layers = []\n",
    "    for encoded_sentence, word_indice in zip(encoded_sentences, indices):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(encoded_sentence.unsqueeze(0), output_hidden_states=True)\n",
    "            hidden_states = outputs['hidden_states']\n",
    "            pooled_output = torch.mean(torch.stack(hidden_states), dim=0)\n",
    "            all_layers.append(pooled_output.squeeze()[word_indice])\n",
    "\n",
    "    # Perform mean pooling on the resulting vectors\n",
    "    word_vector = torch.mean(torch.stack(all_layers), dim=0)\n",
    "\n",
    "    # Convert the word vector to a numpy array\n",
    "    word_vector = word_vector.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd683b5",
   "metadata": {},
   "source": [
    "Evaluating (First with Euclidian distance, then with cosine similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2258da58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "w='toll'\n",
    "\n",
    "# Given arrays\n",
    "array1 = d_sep[f'{w}1']\n",
    "array2 = d_sep[f'{w}2']\n",
    "array3 = d[f'{w}']\n",
    "\n",
    "# Construct the coefficient matrix\n",
    "A = np.vstack([array1, array2]).T\n",
    "\n",
    "# Solve the least squares problem\n",
    "coefficients, residuals, _, _ = np.linalg.lstsq(A, array3, rcond=None)\n",
    "\n",
    "# Calculate the linear combination\n",
    "linear_combination = np.dot(A, coefficients)\n",
    "\n",
    "# Calculate the distance between linear_combination and array3\n",
    "distance = np.linalg.norm(linear_combination - array3)\n",
    "\n",
    "print(\"Coefficients:\", coefficients)\n",
    "print(\"Distance:\", distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f45d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = np.linspace(0.0,10,num=1000)\n",
    "c2 = np.linspace(0.0,10,num=1000)\n",
    "\n",
    "w = 'rák'\n",
    "# Assuming you have three numpy arrays: arr1, arr2, arr3\n",
    "arr1 = d_sep[f'{w}1']\n",
    "arr2 = d_sep[f'{w}2']\n",
    "arr3 = d[f'{w}']\n",
    "maximum = 0\n",
    "a=0\n",
    "b=0\n",
    "for i in c1:\n",
    "    for j in c2:\n",
    "        weighted_sum = i * arr1 + j * arr2\n",
    "        similarity = cosine_similarity(weighted_sum.reshape(1, -1), arr3.reshape(1, -1))\n",
    "        if similarity > maximum:\n",
    "            maximum = similarity\n",
    "            a = i\n",
    "            b = j\n",
    "print(similarity, a, b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
